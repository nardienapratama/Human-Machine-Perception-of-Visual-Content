{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "use_ml_obj = True\n",
    "use_ml_capt = False\n",
    "use_human_labels = False\n",
    "n_jobs = 24\n",
    "embeddings_path = \"/data/outputs_50/finetuning_all-MiniLM-L12-v2_embeddings.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to Perform Region Classification Using Embeddings\n",
    "\n",
    "Sources:\n",
    "- https://imbalanced-learn.org/stable/ensemble.html#boosting\n",
    "\n",
    "Author: Nardiena A. Pratama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dir = \"clsf-v3\"\n",
    "run_code = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install imblearn fairlearn joblib seaborn\n",
    "!pip install wordsegment autocorrect \n",
    "!pip install spacy==3.8.0\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import joblib\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from fairlearn.metrics import demographic_parity_ratio, equalized_odds_ratio\n",
    "\n",
    "\n",
    "from helper_scripts.preprocess import *\n",
    "from helper_scripts.utility_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set AWS Credentials\n",
    "\n",
    "Do not put quotation marks around the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env BUCKET_NAME=aws_bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session using the default credentials (IAM role attached to the instance)\n",
    "session = boto3.Session()\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# Specify your bucket name\n",
    "bucket_name = os.getenv('BUCKET_NAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS = []\n",
    "if use_ml_obj:\n",
    "    ANNOTATIONS.append(\"ml_object_embed\")\n",
    "if use_ml_capt:\n",
    "    ANNOTATIONS.append(\"ml_caption_embed\")\n",
    "\n",
    "if use_human_labels:\n",
    "    ANNOTATIONS.append(\"human_embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ANNOTATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = s3.get_object(Bucket=bucket_name, Key=embeddings_path)\n",
    "csv_content = response['Body'].read().decode('utf-8')\n",
    "data = pd.read_csv(StringIO(csv_content))\n",
    "# Read the embedding columns as arrays\n",
    "data[\"ml_object_embed\"]= data.apply(lambda x: convert_str_to_array(x[\"ml_object_embed\"]), axis=1)\n",
    "data[\"ml_caption_embed\"]= data.apply(lambda x: convert_str_to_array(x[\"ml_caption_embed\"]), axis=1)\n",
    "data[\"human_embed\"]= data.apply(lambda x: convert_str_to_array(x[\"human_embed\"]), axis=1)\n",
    "# data = data.groupby('category').apply(lambda x: x.sample(n=40, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = data[['id', 'category','ml_object_embed','ml_caption_embed', 'human_embed', 'region', 'country', 'income']].copy()\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset['embeddings'] = subset.apply(lambda row: np.concatenate([row[i] for i in ANNOTATIONS]), axis=1)\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_X = subset[['ml_object_embed','ml_caption_embed', 'human_labels_embed']]\n",
    "# subset_X = subset['embeddings']\n",
    "subset_X = subset['embeddings'].apply(lambda x: np.array(x).flatten())\n",
    "subset_X = np.stack(subset_X)\n",
    "subset_y = subset['region'].values\n",
    "categories = subset[['category']]\n",
    "unique_categories = np.unique(categories)\n",
    "unique_regions = np.unique(subset_y)\n",
    "\n",
    "\n",
    "subset_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, cats_train, cats_test = train_test_split(subset_X, subset_y,categories, test_size=0.2, random_state=42, stratify=subset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats_test['category'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RUSBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "assert run_code == True, \"Run code is set to False! Change value to run code below.\"\n",
    "\n",
    "n_runs = 10\n",
    "random_seeds = list(range(n_runs))\n",
    "\n",
    "all_y_predictions = dict()\n",
    "\n",
    "all_scores = dict()\n",
    "all_scores['overall_weighted'] = []\n",
    "all_scores['overall_macro'] = []\n",
    "\n",
    "\n",
    "for label in unique_regions:\n",
    "    all_scores[label] = {'none-average': [], 'group-by': []}\n",
    "\n",
    "all_scores_category = dict()\n",
    "\n",
    "\n",
    "for curr_cat in unique_categories:\n",
    "    all_scores_category[curr_cat] = {'weighted': [], 'macro': [], 'group-by': []}\n",
    "\n",
    "for seed in random_seeds:\n",
    "    print(f\"Seed: {seed}\")\n",
    "    parameters = {\n",
    "        'n_estimators': list(range(50,150,50)),\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.5, 1.0],\n",
    "    }\n",
    "    # f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "    dt_base_learner = DecisionTreeClassifier(random_state = seed, class_weight=\"balanced\")\n",
    "\n",
    "    rusboost = RUSBoostClassifier(estimator=dt_base_learner, sampling_strategy='not minority',\n",
    "                                random_state=seed)\n",
    "    clf = GridSearchCV(rusboost, parameters, scoring=\"f1_weighted\", return_train_score=True, n_jobs=n_jobs)\n",
    "    with joblib.parallel_backend(backend='loky', n_jobs=n_jobs):\n",
    "        clf.fit(X_train,  y_train)\n",
    "    print(\"Grid Search CV done...\")\n",
    "    print(\"Training based on best parameters...\")\n",
    "    real_rusboost = RUSBoostClassifier(estimator=DecisionTreeClassifier(\n",
    "                                    class_weight=clf.best_estimator_.estimator_.class_weight,\n",
    "                                    random_state=seed),\n",
    "                   learning_rate=clf.best_estimator_.learning_rate,                     \n",
    "                   n_estimators=clf.best_estimator_.n_estimators, \n",
    "                   random_state=seed,\n",
    "                   sampling_strategy=clf.best_estimator_.sampling_strategy)\n",
    "    real_rusboost.fit(X_train, y_train)\n",
    "    print(\"Model has been fitted!\")\n",
    "    y_test_predictions = real_rusboost.predict(X_test)\n",
    "    all_y_predictions[seed] = y_test_predictions\n",
    "    \n",
    "    print(np.mean(f1_score(y_test, y_test_predictions, average=None)))\n",
    "    \n",
    "    macro_f1 = f1_score(y_test, y_test_predictions, average='macro')\n",
    "    print(\"overall Class f1: \", macro_f1)\n",
    "    all_scores['overall_macro'].append(macro_f1)\n",
    "    \n",
    "    weighted_f1 = f1_score(y_test, y_test_predictions, average='weighted')\n",
    "    print(\"weighted f1: \", weighted_f1 )\n",
    "    all_scores['overall_weighted'].append(weighted_f1)\n",
    "\n",
    "    #  This approach below or do subset.groupby('y_test').apply(lambda x: f1_score(x['y_test'], x['y_pred']), axis=1)\n",
    "    class_f1 = f1_score(y_test, y_test_predictions, average=None, labels=unique_regions)\n",
    "    print(class_f1)\n",
    "\n",
    "    df = pd.DataFrame({'y_test': y_test, 'y_pred': y_test_predictions})\n",
    "    f1_groupby = df.groupby('y_test').apply(lambda group: f1_score(group['y_test'], group['y_pred'], average='weighted')).to_dict()\n",
    "    \n",
    "    for idx, cls in enumerate(unique_regions):\n",
    "        print(idx)\n",
    "        all_scores[cls]['none-average'].append(class_f1[idx])\n",
    "        all_scores[cls]['group-by'].append(f1_groupby[cls])\n",
    "\n",
    "\n",
    "    # Calculate the F1 score for the current category\n",
    "    print(\"creating df...\")\n",
    "    df = pd.DataFrame({'y_test': y_test, 'y_pred': y_test_predictions, 'category': cats_test['category'].to_list()})\n",
    "    f1_groupby_cat = df.groupby('category').apply(lambda group: f1_score(group['y_test'], group['y_pred'], average='weighted')).to_dict()\n",
    "    print(\"Group-By Weighted f1 cat: \", f1_groupby_cat)\n",
    "    \n",
    "    # Loop through each category to calculate the F1 score for that category\n",
    "    for category in unique_categories:\n",
    "        all_scores_category[category]['group-by'].append(f1_groupby_cat[category])\n",
    "        \n",
    "        # Create a mask to select the data for the current category\n",
    "        category_mask = cats_test['category'] == category\n",
    "        \n",
    "        y_true_category = y_test[category_mask]\n",
    "        y_pred_category = y_test_predictions[category_mask]\n",
    "        \n",
    "        # Calculate the F1 score for the current category\n",
    "        f1 = f1_score(y_true_category, y_pred_category, average='macro')  \n",
    "        print(\"Macro f1 cat: \", f1)\n",
    "        all_scores_category[category]['macro'].append(f1)\n",
    "\n",
    "        # Calculate the F1 score for the current category\n",
    "        f1 = f1_score(y_true_category, y_pred_category, average='weighted')  \n",
    "        print(\"Weighted f1 cat: \", f1)\n",
    "        all_scores_category[category]['weighted'].append(f1)\n",
    "\n",
    "        \n",
    "        print(f\"F1 score for category {category}: {f1}\")\n",
    "    \n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = []\n",
    "# Append relevant terms based on the flags\n",
    "if use_ml_obj:\n",
    "    parts.append('ml_obj')\n",
    "if use_ml_capt:\n",
    "    parts.append('ml_capt')\n",
    "if use_human_labels:\n",
    "    parts.append('human_labels')\n",
    "    \n",
    "\n",
    "# Join the parts with underscores\n",
    "annotations_used_underscore = '_'.join(parts)\n",
    "annotations_used = \" \".join(annotations_used_underscore.split(\"_\"))\n",
    "annotations_used_underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = annotations_used.title()\n",
    "\n",
    "\n",
    "s3_path_all_scores = f'/data/outputs_50/model_outputs/{model_output_dir}/{annotations_used_underscore}_clsf_all_scores.pickle'\n",
    "s3_path_all_scores_category = f'/data/outputs_50/model_outputs/{model_output_dir}/{annotations_used_underscore}_clsf_all_scores_category.pickle'\n",
    "s3_path_all_y_preds = f'/data/outputs_50/model_outputs/{model_output_dir}/{annotations_used_underscore}_clsf_all_y_predictions.pickle'\n",
    "\n",
    "if run_code:\n",
    "    upload_pickle_to_s3(s3, bucket_name, s3_path_all_scores, all_scores)\n",
    "    upload_pickle_to_s3(s3, bucket_name, s3_path_all_scores_category, all_scores_category)\n",
    "    upload_pickle_to_s3(s3, bucket_name, s3_path_all_y_preds, all_y_predictions)\n",
    "\n",
    "else:\n",
    "    all_scores = read_pickle_from_s3(s3, bucket_name, s3_path_all_scores)\n",
    "    all_scores_category = read_pickle_from_s3(s3, bucket_name, s3_path_all_scores_category)\n",
    "    y_test_predictions = read_pickle_from_s3(s3, bucket_name, s3_path_all_y_preds)\n",
    "    y_test_predictions = np.mean(\n",
    "                            np.stack(list(y_test_predictions.values())), axis=0\n",
    "                        )\n",
    "    \n",
    "\n",
    "# Upload to S3\n",
    "buffer = StringIO()\n",
    "\n",
    "# Create the content to upload\n",
    "line = f\"Results for {title}...\\n\"\n",
    "print(line)\n",
    "buffer.write(line + '\\n')\n",
    "\n",
    "line = \"============================== Overall F1 ==============================\"\n",
    "print(line)\n",
    "buffer.write(line + '\\n')\n",
    "line = f\"Overall Weighted Mean: {normal_round(np.mean(all_scores['overall_weighted']),2)}, Standard Deviation: {normal_round(np.std(all_scores['overall_weighted']),2)}\"\n",
    "print(line)\n",
    "buffer.write(line + '\\n')\n",
    "line = f\"Overall Macro Mean: {normal_round(np.mean(all_scores['overall_macro']),2)}, Standard Deviation: {normal_round(np.std(all_scores['overall_macro']),2)}\"\n",
    "print(line)\n",
    "buffer.write(line + '\\n')\n",
    "\n",
    "line = f\"\\n============================= Grouped by Region =============================\"\n",
    "print(line)\n",
    "buffer.write(line + '\\n')\n",
    "\n",
    "for cls in unique_regions:\n",
    "    line = f\"\\n============================== {cls} ==============================\"\n",
    "    print(line)\n",
    "    buffer.write(line + '\\n')\n",
    "    line = f\"None-Average F1 Mean: {normal_round(np.mean(all_scores[cls]['none-average']),2)}, Standard Deviation: {normal_round(np.std(all_scores[cls]['none-average']),2)}\"\n",
    "    print(line)\n",
    "    buffer.write(line + '\\n')\n",
    "    line = f\"Group-By F1 Mean: {normal_round(np.mean(all_scores[cls]['group-by']),2)}, Standard Deviation: {normal_round(np.std(all_scores[cls]['group-by']),2)}\"\n",
    "    print(line)\n",
    "    buffer.write(line + '\\n')\n",
    "\n",
    "line = f\"\\n============================= Grouped by Image Category =============================\"\n",
    "print(line)\n",
    "buffer.write(line + '\\n')\n",
    "\n",
    "for cls in unique_categories:\n",
    "    # print(\"shape:\", len(all_scores_category[cls]), all_scores_category[cls])\n",
    "    line = f\"\\n============================== {cls} ==============================\"\n",
    "    print(line)\n",
    "    buffer.write(line + '\\n')\n",
    "    line = f\"Weighted F1 Mean: {normal_round(np.mean(all_scores_category[cls]['weighted']),2)}, Standard Deviation: {normal_round(np.std(all_scores_category[cls]['weighted']),2)}\"\n",
    "    print(line)\n",
    "    buffer.write(line + '\\n')\n",
    "    line = f\"Macro F1 Mean: {normal_round(np.mean(all_scores_category[cls]['macro']),2)}, Standard Deviation: {normal_round(np.std(all_scores_category[cls]['macro']),2)}\"\n",
    "    print(line)\n",
    "    buffer.write(line + '\\n')\n",
    "    # this should be same as weighted f1 mean\n",
    "    line = f\"Group-By F1 Mean: {normal_round(np.mean(all_scores_category[cls]['group-by']),2)}, Standard Deviation: {normal_round(np.std(all_scores_category[cls]['group-by']),2)}\"\n",
    "    print(line)\n",
    "    buffer.write(line + '\\n')\n",
    "\n",
    "\n",
    "buffer.seek(0)  # Move to the start of the buffer\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3_key = f'/data/outputs_50/model_outputs/{model_output_dir}/{annotations_used_underscore}_clsf_final_results.txt'\n",
    "if run_code:\n",
    "    s3.put_object(Bucket=bucket_name, Key=s3_key, Body=buffer.getvalue())\n",
    "    print(f\"\\nFinal results file created and written successfully into {s3_key}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only applicable to last test run!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_predictions)\n",
    "# Visualize the confusion matrix as a heatmap\n",
    "class_labels = np.unique(subset_y)\n",
    "cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# filename += annotations_used\n",
    "# plt.savefig(f'clsf_figs/clsf_conf_matrix_{annotations_used_underscore}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predictions))\n",
    "print(\"f1 per class: \", f1_score(y_test, y_test_predictions, average=None))\n",
    "\n",
    "print(\"weighted f1: \", f1_score(y_test, y_test_predictions, average='weighted'))\n",
    "\n",
    "s3_file_key = f\"/data/outputs_50/model_outputs/{model_output_dir}/{annotations_used_underscore}_clsf_report.txt\"\n",
    "\n",
    "# Generate classification report and F1 scores\n",
    "classification_report_str = \"Classification Report:\\n\"\n",
    "classification_report_str += classification_report(y_test, y_test_predictions) + \"\\n\"\n",
    "classification_report_str += \"f1 per class: \" + str(f1_score(y_test, y_test_predictions, average=None)) + \"\\n\"\n",
    "classification_report_str += \"weighted f1: \" + str(f1_score(y_test, y_test_predictions, average='weighted')) + \"\\n\"\n",
    "\n",
    "# Upload the report to S3\n",
    "# s3.put_object(\n",
    "#     Bucket=bucket_name,\n",
    "#     Key=s3_file_key,\n",
    "#     Body=classification_report_str,\n",
    "#     ContentType='text/plain'\n",
    "# )\n",
    "\n",
    "# print(f\"Classification report saved to S3 at: s3://{bucket_name}/{s3_file_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in np.unique(y_test):\n",
    "    region_mask = (y_test == region)\n",
    "    positive_predictions = sum(y_test_predictions[region_mask] == region)\n",
    "    print(f\"Region: {region}, Positive Predictions: {positive_predictions}\")\n",
    "print(f\"Demographic parity: {demographic_parity_ratio(y_test, y_test_predictions, sensitive_features=y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from fairlearn.metrics import MetricFrame\n",
    "\n",
    "# some random numbers\n",
    "y_true = y_test\n",
    "y_pred = y_test_predictions\n",
    "sensitive_features = y_true\n",
    "\n",
    "# compute metric frame\n",
    "mf = MetricFrame(metrics=accuracy_score, \n",
    "                  y_true=y_true, y_pred=y_pred, \n",
    "                  sensitive_features=sensitive_features)\n",
    "# print results\n",
    "print(mf.by_group) # series with accuracy for each sensitive group\n",
    "print(mf.difference()) # difference in accuracy between the two sensitive groups\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_directory(local_dir, bucket_name, s3_path):\n",
    "    for root, dirs, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_file_path, local_dir)\n",
    "            s3_file_path = os.path.join(s3_path, relative_path).replace(\"\\\\\", \"/\")  # Replace for S3 compatibility\n",
    "            \n",
    "            print(f\"Uploading {local_file_path} to s3://{bucket_name}/{s3_file_path}\")\n",
    "            s3.upload_file(local_file_path, bucket_name, s3_file_path)\n",
    "\n",
    "\n",
    "local_directory = \"clsf_figs/\"  # Local directory to upload\n",
    "s3_directory = f\"/data/outputs_50/model_outputs/{model_output_dir}/\"  # S3 path where the directory will be uploaded\n",
    "\n",
    "# upload_directory(local_directory, bucket_name, s3_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
