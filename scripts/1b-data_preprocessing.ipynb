{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to Preprocess Annotations\n",
    "\n",
    "Author: Nardiena A. Pratama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wordsegment autocorrect \n",
    "\n",
    "!pip3 install opencv-python\n",
    "!sudo apt-get update && sudo apt-get install ffmpeg libsm6 libxext6  -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy==3.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "\n",
    "from helper_scripts.preprocess import *\n",
    "from helper_scripts.utility_functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set AWS Credentials\n",
    "\n",
    "Do not put quotation marks around the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env BUCKET_NAME=aws_bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session using the default credentials (IAM role attached to the instance)\n",
    "session = boto3.Session()\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# Specify your bucket name and folder path\n",
    "bucket_name = os.getenv('BUCKET_NAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files containing human labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '/data/resultswithgoodworkeronly.csv'\n",
    "response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "csv_content = response['Body'].read().decode('utf-8')\n",
    "human_df = pd.read_csv(StringIO(csv_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Categories and Video IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cat_VID(row):\n",
    "    result = row['Input.image_url'].split(\".jpg\")[0].split(\"/\")[-2:]\n",
    "    category, VID = result[0], result[1]\n",
    "    return category, VID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "human_df['category'] = human_df.apply(lambda row: extract_cat_VID(row)[0], axis=1)\n",
    "human_df['VID'] = human_df.apply(lambda row: extract_cat_VID(row)[1], axis=1)\n",
    "human_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Number of Images Per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict = dict()\n",
    "cat_set = set()\n",
    "for i, j in human_df.iterrows():\n",
    "    category, image_id = (j['Input.image_url'].split(\".jpg\")[0].split(\"/\")[-2::])\n",
    "    cat_img_id = category + \"/\" + image_id\n",
    "    if category not in cat_dict:\n",
    "        cat_dict[category] = 1\n",
    "    else:\n",
    "        if cat_img_id in cat_set:\n",
    "            continue\n",
    "        else:\n",
    "            cat_dict[category] += 1\n",
    "    cat_set.add(cat_img_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = human_df['category'].unique()\n",
    "CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df.groupby(\"VID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_df_cleaned = human_df.groupby(\"VID\").apply(\n",
    "#     lambda group: group[~group.duplicated(subset=columns_of_interest, keep=\"first\")]\n",
    "# ).reset_index(drop=True)\n",
    "# human_df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Average Num of Tokens/Words per Tag and Average Num of Characters per Tag\n",
    "\n",
    "Each worker gives 5-10 tags (words/phrases). We are calculating the average num of tokens/words for each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total words and non-empty tags\n",
    "total_words = 0\n",
    "total_tags = 0\n",
    "\n",
    "columns_of_interest = [col for col in human_df.columns if \"Answer.tag\" in col]\n",
    "print(columns_of_interest)\n",
    "\n",
    "# Fill NaN values with empty strings\n",
    "human_df[columns_of_interest] = human_df[columns_of_interest].fillna(\"\")\n",
    "\n",
    "\n",
    "# Calculate words per row\n",
    "def calculate_total_words_in_row(row):\n",
    "    # Count non-empty cells in the row (tags)\n",
    "    non_empty_cells = [cell for cell in row if cell.strip() != \"\"]\n",
    "    \n",
    "    # Calculate total words in non-empty cells\n",
    "    total_words = sum(len(cell.split()) for cell in non_empty_cells)\n",
    "    \n",
    "    return total_words\n",
    "\n",
    "\n",
    "def replace_duplicates_with_empty(group):\n",
    "    # Keep track of seen tags for this group\n",
    "    seen_tags = set()\n",
    "    \n",
    "    # Create a copy to modify\n",
    "    modified_group = group.copy()\n",
    "    \n",
    "    # For each row in the group\n",
    "    for idx in group.index:\n",
    "        # Get current row's tags\n",
    "        current_tags = group.loc[idx, columns_of_interest]\n",
    "        \n",
    "        # For each tag in current row\n",
    "        for col in columns_of_interest:\n",
    "            tag = current_tags[col]\n",
    "            # If we've seen this tag before in this VID group, replace with empty string\n",
    "            if tag in seen_tags:\n",
    "                modified_group.loc[idx, col] = \"\"\n",
    "            else:\n",
    "                seen_tags.add(tag)\n",
    "    \n",
    "    return modified_group\n",
    "\n",
    "# Apply the function to each VID group\n",
    "human_df = human_df.groupby('VID').apply(replace_duplicates_with_empty).reset_index(drop=True)\n",
    "\n",
    "# Apply the function to each row to compute row-level total words\n",
    "human_df['total_words_in_row'] = human_df[columns_of_interest].apply(calculate_total_words_in_row, axis=1)\n",
    "\n",
    "# Overall average words per row across all rows\n",
    "average_words_per_row = human_df['total_words_in_row'].mean()\n",
    "\n",
    "# Calculate total words and tags for overall stats\n",
    "total_words = sum(len(cell.split()) for col in columns_of_interest for cell in human_df[col] if cell.strip() != \"\")\n",
    "total_non_empty_tags = sum(1 for col in columns_of_interest for cell in human_df[col] if cell.strip() != \"\")\n",
    "average_words_per_non_empty_tag = total_words / total_non_empty_tags if total_non_empty_tags > 0 else 0\n",
    "\n",
    "# Calculate total characters and average characters per non-empty tag\n",
    "total_characters = sum(len(cell) for col in columns_of_interest for cell in human_df[col] if cell.strip() != \"\")\n",
    "average_characters_per_non_empty_tag = total_characters / total_non_empty_tags if total_non_empty_tags > 0 else 0\n",
    "\n",
    "# Results\n",
    "print(f\"Average Words per Non-Empty Tag: {average_words_per_non_empty_tag:.2f}\")\n",
    "print(f\"Average Characters per Non-Empty Tag: {average_characters_per_non_empty_tag:.2f}\")\n",
    "print(f\"Average Words per Worker Entry: {average_words_per_row:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess human labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove noise to get rid of punctuation\n",
    "- make lower case to make consistent\n",
    "- don't remove stop words, like \"on\", \"in\", may be valuable in showing spatial relationships in the image annotations, needed fo contextual nuance\n",
    "\n",
    "- don't lemmatize, or do stemming because valuable context could be stripped away, i.e., \"wash\" and \"washing\" could have different meanings\n",
    "- use autocorrect to fix typos (not perfect)\n",
    "- segment words in case there is no space between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_preprocessing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "assert run_preprocessing == True, \"run_preprocessing is set to False! Setting it to true will run preprocessing functions, which can take up to 50 mins!\"\n",
    "\n",
    "human_preprocessed = preprocessing_human(human_df, 'human_labels', ['Answer.tag1', 'Answer.tag2', 'Answer.tag3',\n",
    "       'Answer.tag4', 'Answer.tag5', 'Answer.tag6', 'Answer.tag7',\n",
    "       'Answer.tag8', 'Answer.tag9', 'Answer.tag10'])\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "human_preprocessed.to_csv(csv_buffer, index=False)\n",
    "\n",
    "\n",
    "file_path = \"/data/outputs_50/human_labels_preprocessed.csv\"\n",
    "s3.put_object(Bucket=bucket_name, Key=file_path, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"DataFrame saved as CSV and uploaded to {file_path} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate labels to one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/data/outputs_50/human_labels_preprocessed.csv\"\n",
    "response = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "csv_content = response['Body'].read().decode('utf-8')\n",
    "human_preprocessed = pd.read_csv(StringIO(csv_content))\n",
    "human_preprocessed['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict = dict()\n",
    "cat_set = set()\n",
    "for i, j in human_preprocessed.iterrows():\n",
    "    category, image_id = (j['Input.image_url'].split(\".jpg\")[0].split(\"/\")[-2::])\n",
    "    cat_img_id = category + \"/\" + image_id\n",
    "    if category not in cat_dict:\n",
    "        cat_dict[category] = 1\n",
    "    else:\n",
    "        if cat_img_id in cat_set:\n",
    "            continue\n",
    "        else:\n",
    "            cat_dict[category] += 1\n",
    "    cat_set.add(cat_img_id)\n",
    "cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(human_preprocessed['Input.image_url'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique IDs in Human Annotations: {len(set(human_preprocessed.VID))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group labels together for each ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "human_preprocessed.drop(human_preprocessed[pd.isna(human_preprocessed['labels'])].index, inplace=True)\n",
    "original_order = human_preprocessed.columns.tolist()\n",
    "\n",
    "# Modify Human DF so that Each VID has Only One Row\n",
    "grouped = human_preprocessed.groupby('VID').agg({\n",
    "    'labels': lambda x: ','.join(x),\n",
    "    **{col: 'first' for col in human_preprocessed.columns if col not in ['VID', 'labels']}\n",
    "}).reset_index()\n",
    "human_preprocessed = grouped[original_order]\n",
    "\n",
    "human_preprocessed['labels'] = human_preprocessed.apply(lambda row: remove_extra_commas(row['labels']), axis=1)\n",
    "print(f\"Unique ids: {len(human_preprocessed.VID.unique())}\")\n",
    "# human_preprocessed.to_csv('data/outputs/human_filtered.csv', index=False)\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "human_preprocessed.to_csv(csv_buffer, index=False)\n",
    "\n",
    "\n",
    "file_path = \"/data/outputs_50/human_labels_grouped_preprocessed.csv\"\n",
    "s3.put_object(Bucket=bucket_name, Key=file_path, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"DataFrame saved as CSV and uploaded to {file_path} successfully.\")\n",
    "\n",
    "\n",
    "human_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve All Image Data Pertaining Region/Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/data/outputs_50'\n",
    "# Initialize variables for pagination\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=folder_path)\n",
    "\n",
    "# List to store all CSV file keys\n",
    "csv_files = []\n",
    "\n",
    "# Iterate through each page of results\n",
    "for page in page_iterator:\n",
    "    for obj in page.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        # Check if the key ends with '.csv' and is directly in the specified folder\n",
    "        if key.endswith('.csv')  and key.startswith(f'{folder_path}/downsampled') and any(map(key.__contains__, CATEGORIES)) and any(map(key.__contains__, [\"imagelink\"])):\n",
    "            csv_files.append(key)\n",
    "print(csv_files)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Read each CSV file into a pandas DataFrame and store in a list\n",
    "downsampled_categories_dataframes = {}\n",
    "for file_key in csv_files:\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    csv_content = response['Body'].read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(csv_content))\n",
    "    df_key = file_key.split(\".csv\")[0].split(folder_path)[-1].split(\"_\")[-1]\n",
    "    downsampled_categories_dataframes[df_key] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataframes = pd.DataFrame()\n",
    "for k, v in downsampled_categories_dataframes.items():\n",
    "    downsampled_categories_dataframes[k]['topics'] = k\n",
    "    downsampled_dataframes = pd.concat([downsampled_dataframes, v], axis=0)    # concatenating along rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataframes.topics.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_preprocessed.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(downsampled_dataframes.id.unique())- set(human_preprocessed.VID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Human Labels DF with Downsampled DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_human_df = pd.merge(downsampled_dataframes,human_preprocessed, how='inner', left_on=['id'], right_on=['VID'])\n",
    "downsampled_human_df = downsampled_human_df[['id', 'category', 'images', 'labels', 'region', 'country', 'income']]\n",
    "downsampled_human_df.rename(columns={'labels': 'human_labels'}, inplace=True)\n",
    "downsampled_human_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_human_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess ML Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ML Captions and Labels For Each Image, Preprocess, Read into DF, and Concatenate into One Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_preprocessing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert run_preprocessing == True, \"run_preprocessing is set to False! Setting it to true will run preprocessing functions, which can take up to 50 mins!\"\n",
    "\n",
    "all_ml_df = pd.DataFrame()\n",
    "CONF_LEVEL = 50\n",
    "for curr_category in CATEGORIES:\n",
    "# curr_category = 'drinking-water'\n",
    "    print(curr_category)\n",
    "    output_dir = f'/data/outputs_{CONF_LEVEL}'\n",
    "    file_key_labels = f'{output_dir}/{curr_category}_ml_labels_dict_12-09-2024.pickle'\n",
    "    file_key_captions = f'{output_dir}/{curr_category}_ml_captions_dict_12-09-2024.pickle'\n",
    "    \n",
    "    caption_results = read_pickle_from_s3(s3, bucket_name, file_key_labels)\n",
    "    caption_results[curr_category] = preprocessing_ml_labels(caption_results[curr_category])\n",
    "\n",
    "    temp_caption_df = pd.DataFrame(caption_results)\n",
    "    temp_caption_df['category'] = curr_category\n",
    "    temp_caption_df.reset_index(inplace=True)\n",
    "    temp_caption_df.rename(columns={curr_category: \"ml_captions\", \"index\": \"id\"}, inplace=True)\n",
    "    \n",
    "    label_results = read_pickle_from_s3(s3, bucket_name, file_key_captions)\n",
    "    label_results[curr_category] = preprocessing_ml_labels(label_results[curr_category])\n",
    "\n",
    "    temp_label_df = pd.DataFrame(label_results)\n",
    "    temp_label_df['category'] = curr_category\n",
    "    temp_label_df.reset_index(inplace=True)\n",
    "    temp_label_df.rename(columns={curr_category: \"ml_labels\", \"index\": \"id\"}, inplace=True)\n",
    "\n",
    "    temp_ml_df = pd.merge(temp_caption_df,temp_label_df, how='inner',on=['id', 'category'])\n",
    "\n",
    "    all_ml_df = pd.concat([all_ml_df,temp_ml_df], axis=0)\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "all_ml_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "\n",
    "file_path = \"/data/outputs_50/ml_labels_preprocessed.csv\"\n",
    "s3.put_object(Bucket=bucket_name, Key=file_path, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"DataFrame saved as CSV and uploaded to {file_path} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/data/outputs_50/ml_labels_preprocessed.csv\"\n",
    "response = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "csv_content = response['Body'].read().decode('utf-8')\n",
    "all_ml_df = pd.read_csv(StringIO(csv_content))\n",
    "all_ml_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Average Number of Words Per Caption and Average Number of Characters Per Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average words per caption\n",
    "all_ml_df['word_count'] = all_ml_df['ml_captions'].apply(lambda x: len(x.split()))\n",
    "average_words_per_caption = all_ml_df['word_count'].mean()\n",
    "\n",
    "# Calculate average characters per caption\n",
    "all_ml_df['char_count'] = all_ml_df['ml_captions'].apply(lambda x: len(x))\n",
    "average_characters_per_caption = all_ml_df['char_count'].mean()\n",
    "\n",
    "# Results\n",
    "print(f\"Average Num of Words per Caption: {average_words_per_caption:.2f}\")\n",
    "print(f\"Average Num of Characters per Caption: {average_characters_per_caption:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Average Number of Words Per Object Label, Average Number of Characters Per Object Label, and Average Num of Object Labels per Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_words_per_label(label_string):\n",
    "    labels = label_string.split(',')\n",
    "    total_words = sum(len(label.split()) for label in labels)\n",
    "    return total_words / len(labels) if len(labels) > 0 else 0\n",
    "\n",
    "def avg_chars_per_label(label_string):\n",
    "    labels = label_string.split(',')\n",
    "    total_chars = sum(len(label) for label in labels)\n",
    "    return total_chars / len(labels) if len(labels) > 0 else 0\n",
    "\n",
    "def avg_words_per_row(label_string):\n",
    "    return len(label_string.split(','))\n",
    "\n",
    "# Calculate metrics\n",
    "all_ml_df['avg_words_per_label'] = all_ml_df['ml_labels'].apply(avg_words_per_label)\n",
    "all_ml_df['avg_chars_per_label'] = all_ml_df['ml_labels'].apply(avg_chars_per_label)\n",
    "all_ml_df['avg_words_per_row'] = all_ml_df['ml_labels'].apply(avg_words_per_row)\n",
    "\n",
    "# Overall averages\n",
    "average_words_per_label = all_ml_df['avg_words_per_label'].mean()\n",
    "average_chars_per_label = all_ml_df['avg_chars_per_label'].mean()\n",
    "average_words_per_row = all_ml_df['avg_words_per_row'].mean()\n",
    "\n",
    "# Results\n",
    "print(f\"Average Num of Words per Object Label: {average_words_per_label:.2f}\")\n",
    "print(f\"Average Num of Characters per Object Label: {average_chars_per_label:.2f}\")\n",
    "print(f\"Average Num of Object Labels per Prediction: {average_words_per_row:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ml_df['ml_labels'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge ML Captions/Labels DF with Downsampled DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_ml_df = pd.merge(all_ml_df,downsampled_dataframes, how='inner',left_on=[\"id\",\"category\"], right_on = [\"id\", \"topics\"] )\n",
    "downsampled_ml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_ml_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(downsampled_ml_df.id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_human_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Downsampled ML and Human Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsamped_all_df = pd.merge(downsampled_ml_df, downsampled_human_df, how=\"inner\")\n",
    "downsamped_all_df = downsamped_all_df[['id', 'category', 'country', 'income', 'region', 'human_labels' ,'ml_captions', 'ml_labels']].copy()\n",
    "downsamped_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique labels\n",
    "print(f\"Number of unique ML object labels in dataset: {len(convert_series_to_dict(downsamped_all_df, 'ml_labels'))}\")\n",
    "print(f\"Number of unique ML caption labels in dataset: {len(convert_series_to_dict(downsamped_all_df, 'ml_captions'))}\")\n",
    "\n",
    "print(f\"Number of unique VIDs in ML dataset: {len(downsamped_all_df.id.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique labels in the human annotations (nonusa)\n",
    "print(f\"Number of unique labels in human dataset: {len(convert_series_to_dict(downsamped_all_df, 'human_labels'))}\")\n",
    "print(f\"Number of unique VIDs in human dataset: {len(downsamped_all_df.id.unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique labels in ML dataset: {len(convert_series_to_dict(downsamped_all_df, 'ml_labels'))}\")\n",
    "print(f\"Number of unique VIDs in ML dataset: {len(downsamped_all_df.id.unique())}\")\n",
    "\n",
    "print(f\"Number of unique labels in ML dataset: {len(convert_series_to_dict(downsamped_all_df, 'ml_captions'))}\")\n",
    "print(f\"Number of unique VIDs in ML dataset: {len(downsamped_all_df.id.unique())}\")\n",
    "\n",
    "print(f\"Number of unique labels in human dataset: {len(convert_series_to_dict(downsamped_all_df, 'human_labels'))}\")\n",
    "print(f\"Number of unique VIDs in human dataset: {len(downsamped_all_df.id.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "downsamped_all_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "\n",
    "file_path = \"/data/outputs_50/final_combined_ml_human.csv\"\n",
    "s3.put_object(Bucket=bucket_name, Key=file_path, Body=csv_buffer.getvalue())\n",
    "\n",
    "print(f\"DataFrame saved as CSV and uploaded to {file_path} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/data/outputs_50/final_combined_ml_human.csv\"\n",
    "response = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "csv_content = response['Body'].read().decode('utf-8')\n",
    "final_combined_ml_human = pd.read_csv(StringIO(csv_content))\n",
    "final_combined_ml_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_ml_human.region.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_ml_human.category.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
