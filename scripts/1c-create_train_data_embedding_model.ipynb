{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to Train Embedding Model\n",
    "\n",
    "In this notebook, we try to train a sentence transformer using the handwashing dataset\n",
    "\n",
    "Author: Nardiena A. Pratama\n",
    "\n",
    "Sources Used:\n",
    "\n",
    "- Source of notebook: https://github.com/huggingface/blog/blob/main/how-to-train-sentence-transformers.md\n",
    "- Additional: https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/95_Training_Sentence_Transformers.ipynb#scrollTo=fwYA76vY2YbZ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wordsegment autocorrect \n",
    "!pip install spacy==3.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf\n",
    "!pip install wandb seaborn\n",
    "!pip install accelerate==0.27.2\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import wandb\n",
    "\n",
    "from datasets import (\n",
    "    Dataset, \n",
    "    DatasetDict\n",
    ")\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    InputExample\n",
    "\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import *\n",
    "\n",
    "\n",
    "from helper_scripts.preprocess import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set AWS Credentials\n",
    "\n",
    "Do not put quotation marks around the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env BUCKET_NAME=aws_bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session using the default credentials (IAM role attached to the instance)\n",
    "session = boto3.Session()\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# Specify your bucket name\n",
    "bucket_name = os.getenv('BUCKET_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV containing ML and Human Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = \"repo/data/outputs_50/final_combined_ml_human.csv\"\n",
    "response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "csv_content = response['Body'].read().decode('utf-8')\n",
    "data_df = pd.read_csv(StringIO(csv_content))\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe containing list of sentences for every image ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper_scripts.preprocess import *\n",
    "\n",
    "expanded_data = []\n",
    "for _, row in data_df.iterrows():\n",
    "    perms = create_combinations_from_list(row, ['ml_labels', 'ml_captions', 'human_labels'], 2)\n",
    "    for perm in perms:\n",
    "        duplicated_row = row.to_dict()\n",
    "        duplicated_row[\"Combination Pair\"] = list(perm)  # Add permutation as a new column\n",
    "        expanded_data.append(duplicated_row)\n",
    "\n",
    "data_df_with_combinations = pd.DataFrame(expanded_data)\n",
    "data_df_with_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.DataFrame({'set': data_df_with_combinations['Combination Pair']})\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = combined_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataframe to dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,     \n",
    "})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"- The Handwashing dataset has {dataset_dict['train'].num_rows} examples.\")\n",
    "print(f\"- Each example is a {type(dataset_dict['train'][0])} with a {type(dataset_dict['train'][0]['set'])} as value.\")\n",
    "print(f\"- Examples look like this: {dataset_dict['train'][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Examples look like this: {dataset_dict['train']['set'][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = []\n",
    "train_data = dataset_dict['train']['set']\n",
    "n_examples = dataset_dict['train'].num_rows\n",
    "\n",
    "for i in range(n_examples):\n",
    "  example = train_data[i]\n",
    "#   print(example)\n",
    "  train_examples.append(InputExample(texts=[example[0], example[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ======================= FINETUNED ========================\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "\n",
    "\n",
    "batch_size=16\n",
    "shuffle=True\n",
    "train_dataloader = DataLoader(train_examples, shuffle=shuffle, batch_size=batch_size)\n",
    "train_loss = MultipleNegativesRankingLoss(model)\n",
    "num_epochs = 10\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: If using wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v1\"\n",
    "training_type = \"finetuning_all-MiniLM-L12-v2\" # distilroberta_jan3_icwsm25\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"ICSWSM-2025-RnR\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"batch_size__train_dataloader\": batch_size,\n",
    "        \"shuffle__train_dataloader\": shuffle,\n",
    "        \"num_examples__train_dataloader\": len(train_dataloader),\n",
    "        \"epochs\": num_epochs,\n",
    "        \"train_loss\" : \"MultipleNegativesRankingLoss\",\n",
    "        \"version\": version,\n",
    "        \"training_type\": training_type\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=warmup_steps\n",
    "          ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"models/{training_type}/{version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Repository to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "local_directory = \"models/\"  # Local directory to upload\n",
    "s3_directory = f\"repo/data/outputs_50/models/\"  # S3 path where the directory will be uploaded\n",
    "\n",
    "upload_directory(local_directory, bucket_name, s3_directory, s3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
